#### Commands

To check java properties
sh-5.1$ java -XshowSettings:properties -version

##### Metric query for loki #####

sum by (tenant, reason) (irate(loki_discarded_samples_total[1m]))

##### Must gather command for logging namespace #####
oc adm must-gather --image=$(oc -n openshift-logging get deployment.apps/cluster-logging-operator -o jsonpath='{.spec.template.spec.containers[?(@.name == "cluster-logging-operator")].image}')

##### Catalog source command  #####
oc adm catalog mirror registry.redhat.io/redhat/certified-operator-index:v4.16 ${LOCAL_REGISTRY}/${LOCAL_REPOSITORY} -a ${LOCAL_SECRET_JSON} --filter-by-os="linux/amd64" --manifests-only

oc image mirror -a ${LOCAL_SECRET_JSON} -f quay.txt

'prodregmirr01.retaildr.mypnbone.in:8443/ocp4.16.40/openshift4-redhat-redhat-operator-index:v4.16'

###############################
##### User add command #####

oc adm policy add-cluster-role-to-user cluster-reader <username>

Corp DC
oc adm policy add-cluster-role-to-user cluster-reader 5195281
oc adm policy add-cluster-role-to-user cluster-reader 5202734
oc adm policy add-cluster-role-to-user cluster-reader 5202689

oc adm policy add-cluster-role-to-user cluster-logging-application-view 5195281
oc adm policy add-cluster-role-to-user cluster-logging-application-view 5202734
oc adm policy add-cluster-role-to-user cluster-logging-application-view 5202689


retail DC
oc adm policy add-cluster-role-to-user cluster-reader 5114517
oc adm policy add-cluster-role-to-user cluster-reader 5224526
oc adm policy add-cluster-role-to-user cluster-reader 5202734

oc adm policy add-cluster-role-to-user cluster-logging-application-view 5114517
oc adm policy add-cluster-role-to-user cluster-logging-application-view 5224526
oc adm policy add-cluster-role-to-user cluster-logging-application-view 5202734

#######################
##### Lokistack config #####
spec: 
  template:
      queryFrontend:
        replicas: 3
      querier:
        replicas: 4
#######################
##### Ldap command #####
 url: ldap://pnb.net/OU=HO-DigitalBankingDivision-51750,OU=HeadOfficeUsers,DC=pnb,DC=net?sAMAccountName?sub?(&(objectclass=*)(|(memberOf=CN=OCP_MBS_PROD,OU=Custom
          Security Groups,DC=pnb,DC=net)))
		  
ldapsearch -x -H ldap://pnb.net:389 -D "pnboneocp@pnb.net" -W -b "OU=HO-DigitalBankingDivision-51750,OU=HeadOfficeUsers,DC=pnb,DC=net" "(sAMAccountName=5180055)"

ldapsearch -x -H ldap://pnb.net:389 -D "pnboneocp@pnb.net" -W -b "OU=HO-DigitalBankingDivision-51750,OU=HeadOfficeUsers,DC=pnb,DC=net" "(memberOf=CN=OCP_MBS_PROD,OU=Custom Security Groups,DC=pnb,DC=net)" cn

ldapsearch -x -H ldap://pnb.net:389 -D "pnboneocp@pnb.net" -W -b "OU=HO-DigitalBankingDivision-51750,OU=HeadOfficeUsers,DC=pnb,DC=net" "(memberOf=CN=OCP_MBS_PROD,OU=Custom Security Groups,DC=pnb,DC=net)" |grep -i mail

==========================================================================================
[oneadmin@bastion ~]$ ldapsearch -x   -H ldap://pnb.net   -D "pnboneocp@pnb.net"   -w "Maypbiz@2025"   -b "OU=HO-DigitalBankingDivision-51750,OU=HeadOfficeUsers,DC=pnb,DC=net"   "(&(objectClass=user)(memberOf=CN=OCP_MBS,OU=Custom Security Groups,DC=pnb,DC=net))"   sAMAccountName mail

========================================
for pod in $(oc get pods -n prod | grep deh | awk '{print $1}'); do
  echo "=== Checking $pod ==="
  oc logs $pod -n prod | awk -v p=$pod 'length($0) > 512000 {print p, NR, length($0)/1024, "KB"}'
done
====================================================================================
instance-2kjpj                                  1/1     Running     0          55d    10.130.1.28    retdcmaster3.retaildc.mypnbone.in   <none>           <none>
instance-fl5dj                                  1/1     Running     0          55d    10.129.0.26    retdcmaster1.retaildc.mypnbone.in   <none>           <none>
instance-rwsqq                                  1/1     Running     0          55d    10.128.1.131   retdcmaster2.retaildc.mypnbone.in   <none>           <none>
kibana-56b4c98666-8z9st                         2/2     Running     0          16d    10.130.1.137   retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-compactor-0                        1/1     Running     0          29h    10.128.1.193   retdcmaster2.retaildc.mypnbone.in   <none>           <none>
logging-loki-distributor-67cbbff455-vq9tj       1/1     Running     0          3d4h   10.130.1.88    retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-distributor-67cbbff455-zc7pl       1/1     Running     0          3d4h   10.128.0.61    retdcmaster2.retaildc.mypnbone.in   <none>           <none>
logging-loki-gateway-7975cbf78c-554gl           2/2     Running     0          3d4h   10.128.0.63    retdcmaster2.retaildc.mypnbone.in   <none>           <none>
logging-loki-gateway-7975cbf78c-wwjk6           2/2     Running     0          3d4h   10.130.1.87    retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-index-gateway-0                    1/1     Running     0          29h    10.130.1.7     retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-index-gateway-1                    1/1     Running     0          29h    10.129.1.76    retdcmaster1.retaildc.mypnbone.in   <none>           <none>
logging-loki-ingester-0                         1/1     Running     0          29h    10.128.1.201   retdcmaster2.retaildc.mypnbone.in   <none>           <none>
logging-loki-ingester-1                         1/1     Running     0          29h    10.130.0.253   retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-ingester-2                         1/1     Running     0          29h    10.128.1.185   retdcmaster2.retaildc.mypnbone.in   <none>           <none>
logging-loki-querier-8bbf558df-4vhkf            1/1     Running     0          3d4h   10.130.1.86    retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-querier-8bbf558df-hfz59            1/1     Running     0          3d4h   10.130.1.92    retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-querier-8bbf558df-sqgs6            1/1     Running     0          3d4h   10.129.1.130   retdcmaster1.retaildc.mypnbone.in   <none>           <none>
logging-loki-query-frontend-56d5dffb9d-pntgx    1/1     Running     0          3d4h   10.130.1.89    retdcmaster3.retaildc.mypnbone.in   <none>           <none>
logging-loki-query-frontend-56d5dffb9d-xxm62    1/1     Running     0          3d4h   10.128.0.62    retdcmaster2.retaildc.mypnbone.in   <none>           <none>

===============================================================================
kind: Deployment
apiVersion: apps/v1
metadata:
  name: deh
  namespace: prod

spec:
  replicas: 12
  selector:
    matchLabels:
      app.kubernetes.io/instance: retail-prod-apps
      app.kubernetes.io/name: deh
    spec:
      nodeSelector:
        application: mobilebanking
      restartPolicy: Always
      initContainers:
        - resources:
            limits:
              cpu: 20m
              memory: 75M
            requests:
              cpu: 10m
              memory: 50M
          terminationMessagePath: /dev/termination-log
          name: appd-agent-attach-java
          command:
            - /bin/sh
            - '-c'
            - 'cp -r /opt/appdynamics/. /opt/appdynamics-java && chown -R 10001:11001 /opt/appdynamics-java ; ls -la /opt/appdynamics-java '
          securityContext:
            privileged: false
            runAsUser: 10001
            runAsGroup: 11001
            runAsNonRoot: false
            readOnlyRootFilesystem: false
            allowPrivilegeEscalation: false
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: appd-agent-repo-java
              mountPath: /opt/appdynamics-java
          terminationMessagePolicy: File
          image: 'retaildc-registry-quay-quay-enterprise.apps.retaildc.mypnbone.in/pnbretailappdynamics/java-agent:25.4.0'
      serviceAccountName: finacle-sa
      imagePullSecrets:
        - name: retail-prod-apps-regcred
      schedulerName: default-scheduler
      terminationGracePeriodSeconds: 30
      securityContext:
        runAsUser: 10001
        runAsGroup: 11001
        fsGroup: 11001
      containers:
        - resources:
            limits:
              cpu: '2'
              memory: 17Gi
            requests:
              cpu: '2'
              memory: 10Gi
          readinessProbe:
            httpGet:
              path: /deh/health
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 245
            timeoutSeconds: 230
            periodSeconds: 230
            successThreshold: 1
            failureThreshold: 3
          terminationMessagePath: /dev/termination-log
          name: deh
          livenessProbe:
            httpGet:
              path: /deh/health
              port: 9090
              scheme: HTTP
            initialDelaySeconds: 250
            timeoutSeconds: 230
            periodSeconds: 230
            successThreshold: 1
            failureThreshold: 3
          env:
            - name: APPDYNAMICS_AGENT_ACCOUNT_ACCESS_KEY
              valueFrom:
                secretKeyRef:
                  name: cluster-agent-secret
                  key: controller-key
            - name: JAVA_OPTS
              value: '-Djava.security.egd=file:/dev/./urandom -Dconfig.server.profile=prod -Dconfig.server.credential.type=basic -Dconfig.server.credential.basic.username=configuser -Dconfig.server.credential.basic.password=B8SBe/GIaIOYWrVVkMXkxw==  -Xms2048m -Xmx8192m -XX:+UseG1GC -XX:MaxGCPauseMillis=200 -XX:CompressedClassSpaceSize=256M -Xlog:gc*,gc+age*=trace,gc+classhisto*=trace,safepoint:file=/opt/log/gc-%%t_%%p.log:tags,time,uptime,level:filecount=20,filesize=50M  -XX:InitiatingHeapOccupancyPercent=45 -XX:MaxMetaspaceSize=500m -XX:+UseStringDeduplication -XX:G1ReservePercent=20 -XX:MaxDirectMemorySize=256m -XX:G1HeapRegionSize=4m -Dsun.net.client.defaultReadTimeout=60000 -Doracle.jdbc.defaultNChar=false -XX:NativeMemoryTracking=detail -Dsun.net.client.defaultConnectTimeout=60000 -Dconfig.server.service=deh -Dconfig.server.url=http://configserver:8080/configserver -Djavax.net.ssl.trustStore=/opt/tomcat/conf/certs/cacerts.jks -Djavax.net.ssl.trustStorePassword=changeit'
            - name: NODE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: spec.nodeName
            - name: NAMESPACE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.namespace
            - name: POD_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: CONTAINER_NAME
              value: deh
            - name: SOURCE
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.name
            - name: MONOLITHIC_HOSTNAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.podIP
            - name: HEALTH_CHECK_PROTOCOL
              value: http
            - name: HEALTH_CHECK_PORT
              value: '9090'
            - name: MONOLITHIC_PORT
              value: '9090'
            - name: CONTEXT_NAME
              value: deh
            - name: TZ
              value: Asia/Kolkata
            - name: APPDYNAMICS_CONTROLLER_SSL_ENABLED
              value: 'false'
            - name: APPDYNAMICS_AGENT_ACCOUNT_NAME
              value: finacle-pnb
            - name: APPDYNAMICS_AGENT_APPLICATION_NAME
              value: RETAIL-DC
            - name: APPDYNAMICS_AGENT_TIER_NAME
              value: deh
            - name: APPDYNAMICS_JAVA_AGENT_REUSE_NODE_NAME_PREFIX
              value: deh
            - name: JAVA_TOOL_OPTIONS
              value: ' -Dappdynamics.agent.accountAccessKey=$(APPDYNAMICS_AGENT_ACCOUNT_ACCESS_KEY) -Dappdynamics.socket.collection.bci.enable=true -Dappdynamics.jvm.shutdown.mark.node.as.historical=false  -Dappdynamics.agent.reuse.nodeName=true -javaagent:/opt/appdynamics-java/javaagent.jar'
            - name: APPDYNAMICS_CONTROLLER_HOST_NAME
              value: finasproc1.mypnbone.in
            - name: APPDYNAMICS_CONTROLLER_PORT
              value: '7777'
            - name: APPDYNAMICS_NETVIZ_AGENT_HOST
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: status.hostIP
            - name: APPDYNAMICS_NETVIZ_AGENT_PORT
              value: '0'
          ports:
            - name: http
              containerPort: 9090
              protocol: TCP
          imagePullPolicy: IfNotPresent
          volumeMounts:
            - name: deh-config-volume
              mountPath: /opt/tomcat/conf/context.xml
              subPath: context.xml
            - name: deh-config-volume
              mountPath: /opt/tomcat/conf/mtls.sh
              subPath: mtls.sh
            - name: deh-config-volume
              mountPath: /opt/tomcat/conf/server.xml
              subPath: server.xml
            - name: deh-config-volume
              mountPath: /opt/extension/hif/config/Custom_HIF_Host.xml
              subPath: Custom_HIF_Host.xml
            - name: deh-config-volume
              mountPath: /opt/extension/hif/config/Custom_HIF_Message.xml
              subPath: Custom_HIF_Message.xml
            - name: deh-config-volume
              mountPath: /opt/extension/hif/config/EB_HIF.xml
              subPath: EB_HIF.xml
            - name: deh-config-volume
              mountPath: /opt/extension/hif/config/HIF_1113_Host.xml
              subPath: HIF_1113_Host.xml
            - name: deh-config-volume
              mountPath: /opt/extension/hif/config/HIF_1113_Host_bkp_2211.xml
              subPath: HIF_1113_Host_bkp_2211.xml
            - name: deh-config-volume
              mountPath: /opt/extension/hif/config/HIF_1113_Message.xml
              subPath: HIF_1113_Message.xml
            - name: deh-secrets-volume
              mountPath: /opt/tomcat/conf/certs/cacerts.jks
              subPath: cacerts.jks
            - name: deh-secrets-volume
              mountPath: /opt/extension/security/AdapterConfig.xml
              subPath: AdapterConfig.xml
            - name: deh-secrets-volume
              mountPath: /opt/extension/security/AdapterConfig.xml_bcp
              subPath: AdapterConfig.xml_bcp
            - name: deh-secrets-volume
              mountPath: /opt/extension/security/KeysConfig.xml
              subPath: KeysConfig.xml
            - name: deh-secrets-volume
              mountPath: /opt/extension/security/KeysConfig.xml_bck
              subPath: KeysConfig.xml_bck
            - name: deh-secrets-volume
              mountPath: /opt/extension/security/PrivateKey.key
              subPath: PrivateKey.key
            - name: deh-secrets-volume
              mountPath: /opt/extension/security/PublicKey.key
              subPath: PublicKey.key
            - name: deh-secrets-volume
              mountPath: /opt/extension/batch/BatchConfig.xml
              subPath: BatchConfig.xml
            - name: deh-secrets-volume
              mountPath: /opt/extension/batch/BatchConfig.xml_bck
              subPath: BatchConfig.xml_bck
            - name: appd-agent-repo-java
              mountPath: /opt/appdynamics-java
          terminationMessagePolicy: File
          image: 'retaildc-registry-quay-quay-enterprise.apps.retaildc.mypnbone.in/pnbprodretail/deh:1116_J17_V9-42'
      hostAliases:
        - ip: 10.192.11.153
          hostnames:
            - pnbfas.com
        - ip: 10.192.18.33
          hostnames:
            - cbsservice.mypnb.in
        - ip: 10.192.2.165
          hostnames:
            - dc-fin-ssc-gbmapp.pnb.co.in
        - ip: 10.192.5.145
          hostnames:
            - pnbupi.in
        - ip: 10.192.233.25
          hostnames:
            - efrmneoprodapp.pnb.net
        - ip: 10.192.5.222
          hostnames:
            - pnbimps.in
        - ip: 10.192.20.116
          hostnames:
            - pnbapps.mypnb.in
        - ip: 10.192.35.59
          hostnames:
            - pnbfinacledc.pnb.co.in
        - ip: 10.192.21.162
          hostnames:
            - www.pnbcard.in
        - ip: 10.192.20.101
          hostnames:
            - aafiu.mypnb.in
        - ip: 10.192.11.50
          hostnames:
            - depository.pnbindia.in
      serviceAccount: finacle-sa
      volumes:
        - name: deh-config-volume
          configMap:
            name: deh-configmap
            defaultMode: 420
        - name: deh-secrets-volume
          secret:
            secretName: deh-secrets
            defaultMode: 420
        - name: data
          emptyDir: {}
        - name: appd-agent-repo-java
          emptyDir: {}
      dnsPolicy: ClusterFirst
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 50%
      maxSurge: 50%
  revisionHistoryLimit: 10
  progressDeadlineSeconds: 600







GRPC call: /csi.v1.Controller/CreateVolume
I1128 09:33:34.847215 1 connection.go:194] GRPC request: {"capacity_range":{"required_bytes":1073741824},"name":"pvc-61e0ea4b-382d-4d3f-b35e-b75c05ad4734","parameters":{"connectionType":"fc","poolID":"9","portID":"CL9-C,CLA-C,CLB-C,CLC-C","serialNumber":"50906"},"secrets":"***stripped***","volume_capabilities":[{"AccessType":{"Mount":{"fs_type":"xfs"}},"access_mode":{"mode":1}}]}
I1128 09:33:34.856195 1 connection.go:200] GRPC response: {}
I1128 09:33:34.856202 1 connection.go:201] GRPC error: rpc error: code = Internal desc = Storage could not be connected : [HSPC0x0000101a] the REST server is unavailable : wait until the REST server is ready
I1128 09:33:34.856214 1 controller.go:816] CreateVolume failed, supports topology = false, node selected false => may reschedule = false => state = Finished: rpc error: code = Internal desc = Storage could not be connected : [HSPC0x0000101a] the REST server is unavailable : wait until the REST server is ready
I1128 09:33:34.856234 1 controller.go:1075] Final error received, removing PVC 61e0ea4b-382d-4d3f-b35e-b75c05ad4734 from claims in progress
W1128 09:33:34.856243 1 controller.go:934] Retrying syncing claim "61e0ea4b-382d-4d3f-b35e-b75c05ad4734", failure 17
E1128 09:33:34.856255 1 controller.go:957] error syncing claim "61e0ea4b-382d-4d3f-b35e-b75c05ad4734": failed to provision volume with StorageClass "sc-hitachi-csi": rpc error: code = Internal desc = Storage could not be connected : [HSPC0x0000101a] the REST server is unavailable : wait until the REST server is ready
I1128 09:33:34.856305 1 event.go:298] Event(v1.ObjectReference{Kind:"PersistentVolumeClaim", Namespace:"kube-system", Name:"test", UID:"61e0ea4b-382d-4d3f-b35e-b75c05ad4734", APIVersion:"v1", ResourceVersion:"136848686", FieldPath:""}): type: 'Warning' reason: 'ProvisioningFailed' failed to provision volume with StorageClass "sc-hitachi-csi": rpc error: code = Internal desc = Storage could not be connected : [HSPC0x0000101a] the REST server is unavailable : wait until the REST server is ready


Bucket Name		             Volume-Size	    Namespace-Url	                                               accesskey	       secretkey	                        Access
pnb1corpretail-nonprod		6 TB	          https://pnb1corpretail-nonprod.pnbone-01.hcpdr.obc.co.in	     cG5ib25lLTAx	     	                                  Read/Write
pnb1retail-prod		        36 TB	          https://pnb1retail-prod.pnbone-01.hcpdr.obc.co.in	             cG5ib25lLTAx	     	                                  Read/Write
pnb1corpl-prod		        12 TB	          https://pnb1corpl-prod.pnbone-01.hcpdr.obc.co.in	             cG5ib25lLTAx	     	                                  Read/Write
========================
apiVersion: monitoring.coreos.com/v1
kind: PrometheusRule
metadata:
  name: collector-error-alert
  namespace: openshift-logging
  labels:
    release: cluster-observability-operator
spec:
  groups:
  - name: collector-logs
    rules:
    - alert: CollectorLogErrorsDetected
      expr: |
        sum by (pod) (
          rate({namespace="openshift-logging", container="collector"} |= "error" [5m])
        ) > 0
      for: 2m
      labels:
        severity: warning
      annotations:
        summary: "Error logs found in collector pod"
        description: "Errors were detected in collector pod logs for the past 5 minutes."
==============================================
Dear Team,

As per the screenshot you shared, we can see that the retention period is configured as 5 days. However, when checking in the log console, we still observe logs older than 5 days (in fact, more than 30 days). Could you please verify and confirm whether the retention policy set at the storage level is functioning as expected?

Additionally, under "Retention options", we noticed the setting for "custom metadata options for objects under retention". The available options are 
1: add 
2: add, delete, and replace.

Currently, it appears to be set to add. Could you clarify whether this setting is related to bucket policies for handling log data? If so, we request that it be configured to add, delete & replace so that old log data can be properly deleted in accordance with the retention policy.


================================Source:
master4.nonprod.mypnbone.in		10.192.7.12
master5.nonprod.mypnbone.in		10.192.7.13
master6.nonprod.mypnbone.in		10.192.7.14

Destination:
10.192.30.192
10.192.30.193
10.192.30.194
10.192.30.195

Port: 443

Error:
Message:               Progressing: Unable to apply resources: unable to sync storage configuration: RequestError: send request failed
caused by: Head "https://pnbone-01.hcpdr.obc.co.in/pnb1-nonprod": dial tcp 10.192.30.193:443: i/o timeout, requeuing


=================================================
[pnbone@retdrbastion ~]$ nmap --script ssl-enum-ciphers -p 443 pnbone-prod-retail.hcpdc.obc.co.in  -Pn
Starting Nmap 7.92 ( https://nmap.org ) at 2025-09-20 12:37 IST
Nmap scan report for pnbone-prod-retail.hcpdc.obc.co.in (10.209.19.87)
Host is up (0.0017s latency).
Other addresses for pnbone-prod-retail.hcpdc.obc.co.in (not scanned): 10.209.19.84 10.209.19.85 10.209.19.86
rDNS record for 10.209.19.87: dr-retail-prod.dr-pnbone-prod-retail.hcpdc.obc.co.in

PORT    STATE SERVICE
443/tcp open  https
| ssl-enum-ciphers:
|   TLSv1.2:
|     ciphers:
|       TLS_DHE_RSA_WITH_AES_256_CBC_SHA (dh 2048) - A
|       TLS_DHE_RSA_WITH_AES_128_CBC_SHA (dh 2048) - A
|       TLS_RSA_WITH_AES_128_CBC_SHA (rsa 2048) - A
|       TLS_RSA_WITH_AES_256_CBC_SHA (rsa 2048) - A
|     compressors:
|       NULL
|     cipher preference: server
|   TLSv1.3:
|     ciphers:
|       TLS_AKE_WITH_AES_128_GCM_SHA256 (secp256r1) - A
|     cipher preference: indeterminate
|     cipher preference error: Too few ciphers supported
|_  least strength: A

Nmap done: 1 IP address (1 host up) scanned in 0.30 seconds
[pnbone@retdrbastion ~]$

[pnbone@retdcbastion busy]$ nmap --script ssl-enum-ciphers -p 443 pnbone-01.hcpdr.obc.co.in -Pn
Starting Nmap 7.92 ( https://nmap.org ) at 2025-09-20 12:32 IST
Nmap scan report for pnbone-01.hcpdr.obc.co.in (10.192.30.191)
Host is up (0.0044s latency).
Other addresses for pnbone-01.hcpdr.obc.co.in (not scanned): 10.192.30.194 10.192.30.193 10.192.30.192
rDNS record for 10.192.30.191: *.hcpdr.obc.co.in

PORT    STATE SERVICE
443/tcp open  https
| ssl-enum-ciphers:
|   TLSv1.2:
|     ciphers:
|       TLS_DHE_RSA_WITH_AES_256_CBC_SHA (dh 2048) - A
|       TLS_DHE_RSA_WITH_AES_128_CBC_SHA (dh 2048) - A
|       TLS_RSA_WITH_AES_128_CBC_SHA (rsa 2048) - A
|       TLS_RSA_WITH_AES_256_CBC_SHA (rsa 2048) - A
|     compressors:
|       NULL
|     cipher preference: server
|_  least strength: A

Nmap done: 1 IP address (1 host up) scanned in 0.50 seconds
[pnbone@retdcbastion busy]$

=================================================================
https://access.redhat.com/articles/5348961
=================================================================


Port 10250 is used by the Kubelet in Kubernetes for internal API communication. It typically serves:

Health checks

Pod logs

Exec and attach commands

Metrics and status queries

This traffic is not browser-based, and HSTS (HTTP Strict Transport Security) is a browser security feature. HSTS only applies to HTTPS web traffic accessed via browsers — not to internal service-to-service communication over TCP or REST APIs used by Kubernetes components.
                    
=======================================================
ssh-ed25519 AAAAC3NzaC1lZDI1NTE5AAAAILOs19s+4Br+227jjA8oCjglCdCtbaQhWMYl+BzYcg7s pnb\v-01010033@HO39990FIN00023
====================================
S.No.

Date

Division

System / Process

QSA Comments

4

17-09-2025

DBTD

PNB ONE BIZ

 

OCP – Hardening evidence will be required as per https://community.ibm.com/community/user/blogs/paul-bastide/2023/11/21/configuring-a-pci-dss-compliant-openshift-containe

==============================================================================================
https://docs.redhat.com/en/documentation/openshift_container_platform/4.18/html/security_and_compliance/file-integrity-operator#troubleshooting-file-integrity-operator

[oneadmin@bastion ~]$ oc get fileintegritynodestatuses.fileintegrity.openshift.io -n openshift-file-integrity
[oneadmin@bastion ~]$ oc get fileintegrities.fileintegrity.openshift.io -n openshift-file-integrity
NAME                  AGE
fileintegrity         21h
infra-fileintegrity   19h
[oneadmin@bastion ~]$
[oneadmin@bastion ~]$ oc get fileintegritynodestatuses.fileintegrity.openshift.io -n openshift-file-integrity
NAME                                        NODE                          STATUS
fileintegrity-worker1.nonprod.mypnbone.in   worker1.nonprod.mypnbone.in   Failed
fileintegrity-worker2.nonprod.mypnbone.in   worker2.nonprod.mypnbone.in   Succeeded
fileintegrity-worker3.nonprod.mypnbone.in   worker3.nonprod.mypnbone.in   Succeeded
fileintegrity-worker4.nonprod.mypnbone.in   worker4.nonprod.mypnbone.in   Succeeded
fileintegrity-worker5.nonprod.mypnbone.in   worker5.nonprod.mypnbone.in   Succeeded
[oneadmin@bastion ~]$
[oneadmin@bastion ~]$ oc get fileintegritynodestatuses.fileintegrity.openshift.io/fileintegrity-worker1.nonprod.mypnbone.in -n openshift-file-integrity -ojsonpath='{.results}' | jq -r
[
  {
    "condition": "Succeeded",
    "lastProbeTime": "2025-09-26T07:22:48Z"
  },
  {
    "condition": "Failed",
    "filesChanged": 1,
    "lastProbeTime": "2025-09-26T07:28:15Z",
    "resultConfigMapName": "aide-fileintegrity-worker1.nonprod.mypnbone.in-failed",
    "resultConfigMapNamespace": "openshift-file-integrity"
  }
]
[oneadmin@bastion ~]$
[oneadmin@bastion ~]$ oc describe cm aide-fileintegrity-worker1.nonprod.mypnbone.in-failed  -n openshift-file-integrity
Name:         aide-fileintegrity-worker1.nonprod.mypnbone.in-failed
Namespace:    openshift-file-integrity
Labels:       file-integrity.openshift.io/node=worker1.nonprod.mypnbone.in
              file-integrity.openshift.io/owner=fileintegrity
              file-integrity.openshift.io/result-log=
Annotations:  file-integrity.openshift.io/files-added: 0
              file-integrity.openshift.io/files-changed: 1
              file-integrity.openshift.io/files-removed: 0

Data
====
integritylog:
----
Start timestamp: 2025-09-26 07:28:35 +0000 (AIDE 0.16)
AIDE found differences between database and filesystem!!

Summary:
  Total number of entries:  36247
  Added entries:              0
  Removed entries:            0
  Changed entries:            1

---------------------------------------------------
Changed entries:
---------------------------------------------------

f   ...    .C... : /hostroot/etc/resolv.conf

---------------------------------------------------
Detailed information about changes:
---------------------------------------------------

File: /hostroot/etc/resolv.conf
  SHA512   : soG6u6KB+Jdb0jD8zj87m5RGrFRgRNV5 | PxPKqw9nsunZbaV3kzkcA9VBsZfZ5o/E
             aD5upRmki+weH03IwEUVPFEsq09oikD4 | 1IoayvwXBfiqnew30noSBlsBVdNOC7Dk
             4l4VFRZUq5EBrX+XT4ivBA==         | ZtQpTcR93nvSZcj7GurOYQ==


---------------------------------------------------
The attributes of the (uncompressed) database(s):
---------------------------------------------------

/hostroot/etc/kubernetes/aide.db.gz
  MD5      : jGJswDbetI4+5BXT5K3PCQ==
  SHA1     : otHStcS1C8gpLxboiXqjmNQgQK0=
  RMD160   : PrjQ5/pJQpnjIYuJk/zsjZEIuUY=
  TIGER    : yDt/6DiLtzDElcL59/T7vc29Mku1DPgR
  SHA256   : 3AToUHGIxH7TKJeH5lzCYVOOPS6UP+HI
             9JmakKI1KOo=
  SHA512   : 88elRmTHqHoIjJ1GWag34UBn401o33Kk
             q7CHcXfWW1KbMuIWeBqog7YOcZCa/X4D
             /UH+Nu8zDU7FjrV3DSElsw==


End timestamp: 2025-09-26 07:29:39 +0000 (run time: 1m 4s)


BinaryData
====

Events:  <none>
[oneadmin@bastion ~]$
[oneadmin@bastion ~]$ oc get fileintegritynodestatuses.fileintegrity.openshift.io -n openshift-file-integrity -o json | jq '.items[] | {node: .nodeName, status: .status, results: .results}'
[oneadmin@bastion ~]$ oc get configmap -n openshift-file-integrity | grep failed
aide-fileintegrity-worker1.nonprod.mypnbone.in-failed   1      20h
[oneadmin@bastion ~]$ oc logs -n openshift-file-integrity -l app=aide-fileintegrity
Defaulted container "daemon" out of: daemon, check-folder (init)
Defaulted container "daemon" out of: daemon, check-folder (init)
Defaulted container "daemon" out of: daemon, check-folder (init)
Defaulted container "daemon" out of: daemon, check-folder (init)
Defaulted container "daemon" out of: daemon, check-folder (init)
2025-09-26T07:32:37Z: aide check returned status 0
2025-09-26T07:32:57Z: running aide check
2025-09-26T07:33:54Z: aide check returned status 0
2025-09-26T07:34:14Z: running aide check
2025-09-26T07:35:02Z: aide check returned status 0
2025-09-26T07:35:22Z: running aide check
2025-09-26T07:36:20Z: aide check returned status 0
2025-09-26T07:36:40Z: running aide check
2025-09-26T07:37:27Z: aide check returned status 0
2025-09-26T07:37:47Z: running aide check
2025-09-26T07:31:40Z: aide check returned status 0
2025-09-26T07:32:00Z: running aide check
2025-09-26T07:33:12Z: aide check returned status 0
2025-09-26T07:33:32Z: running aide check
2025-09-26T07:34:42Z: aide check returned status 0
2025-09-26T07:35:02Z: running aide check
2025-09-26T07:36:13Z: aide check returned status 0
2025-09-26T07:36:33Z: running aide check
2025-09-26T07:37:44Z: aide check returned status 0
2025-09-26T07:38:04Z: running aide check
2025-09-26T07:34:59Z: aide check returned status 0
2025-09-26T07:35:19Z: running aide check
2025-09-26T07:35:43Z: aide check returned status 0
2025-09-26T07:36:03Z: running aide check
2025-09-26T07:36:27Z: aide check returned status 0
2025-09-26T07:36:47Z: running aide check
2025-09-26T07:37:12Z: aide check returned status 0
2025-09-26T07:37:32Z: running aide check
2025-09-26T07:37:56Z: aide check returned status 0
2025-09-26T07:38:16Z: running aide check
2025-09-26T07:35:09Z: running aide check
2025-09-26T07:35:32Z: aide check returned status 0
2025-09-26T07:35:52Z: running aide check
2025-09-26T07:36:13Z: aide check returned status 0
2025-09-26T07:36:33Z: running aide check
2025-09-26T07:36:55Z: aide check returned status 0
2025-09-26T07:37:15Z: running aide check
2025-09-26T07:37:37Z: aide check returned status 0
2025-09-26T07:37:57Z: running aide check
2025-09-26T07:38:18Z: aide check returned status 0
2025-09-26T07:32:25Z: aide check returned status 4
2025-09-26T07:32:45Z: running aide check
2025-09-26T07:33:46Z: aide check returned status 4
2025-09-26T07:34:06Z: running aide check
2025-09-26T07:35:07Z: aide check returned status 4
2025-09-26T07:35:27Z: running aide check
2025-09-26T07:36:31Z: aide check returned status 4
2025-09-26T07:36:51Z: running aide check
2025-09-26T07:37:52Z: aide check returned status 4
2025-09-26T07:38:12Z: running aide check
[oneadmin@bastion ~]$ oc -n openshift-file-integrity get pods -owide
NAME                                      READY   STATUS    RESTARTS      AGE   IP             NODE                          NOMINATED NODE   READINESS GATES
aide-fileintegrity-4thdn                  1/1     Running   0             21h   10.129.5.43    worker5.nonprod.mypnbone.in   <none>           <none>
aide-fileintegrity-4wj57                  1/1     Running   0             21h   10.129.3.132   worker1.nonprod.mypnbone.in   <none>           <none>
aide-fileintegrity-fqhwk                  1/1     Running   0             21h   10.131.2.122   worker3.nonprod.mypnbone.in   <none>           <none>
aide-fileintegrity-pjpdj                  1/1     Running   0             21h   10.128.5.95    worker2.nonprod.mypnbone.in   <none>           <none>
aide-fileintegrity-vw2t8                  1/1     Running   0             21h   10.130.5.125   worker4.nonprod.mypnbone.in   <none>           <none>
aide-infra-fileintegrity-8flqh            0/1     Pending   0             20h   <none>         <none>                        <none>           <none>
aide-infra-fileintegrity-9wbsk            0/1     Pending   0             20h   <none>         <none>                        <none>           <none>
aide-infra-fileintegrity-khlhl            0/1     Pending   0             20h   <none>         <none>                        <none>           <none>
file-integrity-operator-7f4d7d8d6-6njxd   1/1     Running   2 (24h ago)   24h   10.131.2.86    worker3.nonprod.mypnbone.in   <none>           <none>
[oneadmin@bastion ~]$
[oneadmin@bastion ~]$ oc -n openshift-file-integrity logs aide-fileintegrity-4wj57
[oneadmin@bastion ~]$ oc explain fileintegrity.spec
GROUP:      fileintegrity.openshift.io
KIND:       FileIntegrity
VERSION:    v1alpha1

FIELD: spec <Object>

DESCRIPTION:
    FileIntegritySpec defines the desired state of FileIntegrity

FIELDS:
  config        <Object> -required-
    FileIntegrityConfig defines the name, namespace, and data key for an AIDE
    config to use for integrity checking.

  debug <boolean>
    <no description>

  nodeSelector  <map[string]string>
    <no description>

  tolerations   <[]Object>
    Specifies tolerations for custom taints. Defaults to allowing scheduling on
    master and infra nodes.


[oneadmin@bastion ~]$
[oneadmin@bastion ~]$ oc explain fileintegrity.spec.config
GROUP:      fileintegrity.openshift.io
KIND:       FileIntegrity
VERSION:    v1alpha1

FIELD: config <Object>

DESCRIPTION:
    FileIntegrityConfig defines the name, namespace, and data key for an AIDE
    config to use for integrity checking.

FIELDS:
  gracePeriod   <integer>
    Time between individual aide scans

  initialDelay  <integer>
    InitialDelaySeconds is the number of seconds to wait before the first scan.
    It is an optional field, and if not specified, the operator will default to
    0

  key   <string>
    The key that contains the actual AIDE configuration in a configmap specified
    by Name and Namespace. Defaults to aide.conf

  maxBackups    <integer>
    The maximum number of AIDE database and log backups (leftover from the
    re-init process) to keep on a node.
    Older backups beyond this number are automatically pruned by the daemon.

  name  <string>
    Name of a configMap that contains custom AIDE configuration. A default
    configuration would be created if omitted.

  namespace     <string>
    Namespace of a configMap that contains custom AIDE configuration. A default
    configuration would be created if omitted.




--data-urlencode 'password=p@king1'
=================================

1. cluster log forwarding

[oneadmin@bastion ~]$ oc get clf -A
NAMESPACE           NAME       AGE
openshift-logging   instance   2y141d

=======

2. network policy 


[oneadmin@bastion ~]$ oc get networkpolicies.networking.k8s.io
NAME                              POD-SELECTOR                   AGE
allow-from-openshift-ingress      <none>                         2y155d
allow-from-openshift-monitoring   <none>                         2y155d
allow-same-namespace              <none>                         2y155d
istio-expose-route-full-install   maistra.io/expose-route=true   2y26d
istio-mesh-full-install           <none>                         2y26d

=======

3. configure an alternative identity provider


[oneadmin@bastion ~]$ oc get oauth cluster -oyaml
apiVersion: config.openshift.io/v1
kind: OAuth
metadata:
  name: cluster
spec:
  identityProviders:
  - htpasswd:
      fileData:
        name: htpass-secret
    mappingMethod: claim
    name: PNB_HTpasswd
    type: HTPasswd
  - ldap:
      attributes:
        email:
        - mail
        id:
        - sAMAccountName
        name:
        - PNBONEOCP
        preferredUsername:
        - sAMAccountName
      bindDN: pnboneocp@pnb.net
      bindPassword:
        name: ldap-secret
      insecure: true
      url: ldap://pnb.net/OU=HO-DigitalBankingDivision-51750,OU=HeadOfficeUsers,DC=pnb,DC=net?sAMAccountName?sub?(&(objectclass=*)(|(memberOf=CN=OCP_MBS,OU=Custom
        Security Groups,DC=pnb,DC=net)))
    mappingMethod: claim
    name: PNB_NONPROD_LDAP
    type: LDAP
	
=======
	
4. Removing the kubeadmin user

 kubeadmin user Removed
 
=======

5. File Integrity	

[oneadmin@bastion ~]$ oc get fileintegrities.fileintegrity.openshift.io  -A
NAMESPACE                  NAME                   AGE
openshift-file-integrity   master-fileintegrity   2d22h
openshift-file-integrity   worker-fileintegrity   2d19h
[oneadmin@bastion ~]$ oc get fileintegritynodestatuses.fileintegrity.openshift.io  -A
NAMESPACE                  NAME                                               NODE                          STATUS
openshift-file-integrity   worker-fileintegrity-worker1.nonprod.mypnbone.in   worker1.nonprod.mypnbone.in   Succeeded
openshift-file-integrity   worker-fileintegrity-worker2.nonprod.mypnbone.in   worker2.nonprod.mypnbone.in   Succeeded
openshift-file-integrity   worker-fileintegrity-worker3.nonprod.mypnbone.in   worker3.nonprod.mypnbone.in   Succeeded
openshift-file-integrity   worker-fileintegrity-worker4.nonprod.mypnbone.in   worker4.nonprod.mypnbone.in   Succeeded
openshift-file-integrity   worker-fileintegrity-worker5.nonprod.mypnbone.in   worker5.nonprod.mypnbone.in   Succeeded
[pnbone@retdrbastion ~]$ oc get fileintegritynodestatuses.fileintegrity.openshift.io -ojsonpath='{.items[*].results}' | jq
[
  {
    "condition": "Succeeded",
    "lastProbeTime": "2025-09-26T13:04:38Z"
  },
  {
    "condition": "Failed",
    "filesChanged": 1,
    "lastProbeTime": "2025-10-08T05:29:20Z",
    "resultConfigMapName": "aide-worker-fileintegrity-retdrmaster1.retaildr.mypnbone.in-failed",
    "resultConfigMapNamespace": "openshift-file-integrity"
  }
]
[
  {
    "condition": "Succeeded",
    "lastProbeTime": "2025-09-26T13:04:26Z"
  },
  {
    "condition": "Failed",
    "filesChanged": 3,
    "lastProbeTime": "2025-10-08T05:29:18Z",
    "resultConfigMapName": "aide-worker-fileintegrity-retdrmaster2.retaildr.mypnbone.in-failed",
    "resultConfigMapNamespace": "openshift-file-integrity"
  }
]
[
  {
    "condition": "Succeeded",
    "lastProbeTime": "2025-09-26T13:04:26Z"
  },
  {
    "condition": "Failed",
    "filesChanged": 1,
    "lastProbeTime": "2025-10-08T05:29:16Z",
    "resultConfigMapName": "aide-worker-fileintegrity-retdrmaster3.retaildr.mypnbone.in-failed",
    "resultConfigMapNamespace": "openshift-file-integrity"
  }
]
[pnbone@retdrbastion ~]$
[pnbone@retdrbastion ~]$ oc get fileintegritynodestatuses.fileintegrity.openshift.io/worker-fileintegrity-retdrmaster1.retaildr.mypnbone.in -ojsonpath='{.results}' | jq -r
[
  {
    "condition": "Succeeded",
    "lastProbeTime": "2025-09-26T13:04:38Z"
  },
  {
    "condition": "Failed",
    "filesChanged": 1,
    "lastProbeTime": "2025-10-08T05:34:20Z",
    "resultConfigMapName": "aide-worker-fileintegrity-retdrmaster1.retaildr.mypnbone.in-failed",
    "resultConfigMapNamespace": "openshift-file-integrity"
  }
]
[pnbone@retdrbastion ~]$

==================================================
# Count logs from all pods in 1 hour (approx size)
oc get pods -A -o jsonpath='{range .items[*]}{.metadata.namespace}{" "}{.metadata.name}{"\n"}{end}' \
| head -n 50 \
| while read ns pod; do
  echo "---- $ns/$pod ----"
  oc logs -n $ns $pod --since=1h | wc -c
done | awk '{sum += $1} END {print "Approx total bytes in 1h (sample): " sum/1024/1024 " MB"}'
============================================
https://console-openshift-console.apps.retaildc.mypnbone.in/k8s/ns/openshift-file-integrity/pods/aide-worker-fileintegrity-9pshw/vulnerabilities
https://console-openshift-console.apps.corpdc.mypnbone.in/k8s/ns/openshift-console/pods/console-7f456c6775-fdmt7/vulnerabilities
https://console-openshift-console.apps.nonprod.mypnbone.in/k8s/ns/ocp-etcd-backup/pods/openshift-backup-29295030-c96qj/vulnerabilities
https://console-openshift-console.apps.corpdr.mypnbone.in/k8s/ns/openshift-monitoring/pods/alertmanager-main-0/vulnerabilities
https://console-openshift-console.apps.retaildr.mypnbone.in/k8s/ns/openshift-logging/pods/cluster-logging-operator-77fd7db485-f765s/vulnerabilities
